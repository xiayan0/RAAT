# Copyright (c) ByteDance, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
Mostly copy-paste from DINO and timm library:
https://github.com/facebookresearch/dino
https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from functools import partial
from timm.models.layers import to_2tuple

# from einops import einops
# from timm.models.efficientvit_mit import val2tuple
# from torch.cuda.amp import autocast

# from .efficientvit.models.nn.act import build_act
# from .efficientvit.models.nn.norm import build_norm
# from .efficientvit.models.utils import get_same_padding, list_sum, resize, val2list, val2tuple


# import tensorflow as tf
# from .efficientvit import attention_utils as attn_utils
# import logging
# import string
#
# import torch.nn as nn
# import torch
# import torch.nn.functional as F
# import copy
# import math
# import numpy as np
# from typing import Optional, List
# from torch import nn, Tensor
# import torch
# import torch.nn.functional as F
# import copy
# import math
# import math
# from typing import Optional
#
# import math
# from typing import Optional
#
# import torch
# from torch import nn, Tensor
# from torch.nn.init import trunc_normal_


# #MAXVIT需要导入的包
# import string
# import tensorflow as tf
# from .maxvit import attention_utils as attn_utils
# from .maxvit import common_ops as ops
# from .maxvit import hparam_configs
# from absl import logging
# import functools

import re
import copy

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    # if (mean < a - 2 * std) or (mean > b + 2 * std):
    #     warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
    #                   "The distribution of values may be incorrect.",
    #                   stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    # work with diff dim tensors, not just 2D ConvNets
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = keep_prob + \
                    torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


#原始注意力
class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=None):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        # add by wxd
        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        all_head_dim = head_dim * self.num_heads
        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))
            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))
        else:
            self.q_bias = None
            self.v_bias = None
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    # self.flatnn=FocalModulation()
    def forward(self, x, len_t=None, x_rel_pos_bias=None, return_attention=True, mask=None):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(
                self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        if x_rel_pos_bias is not None:
            attn = attn + x_rel_pos_bias.unsqueeze(0)
        if mask is not None:
            attn = attn.masked_fill(~mask, float('-inf'), )
        attn_nosf = attn
        attn = attn.softmax(dim=-1)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        if return_attention:
            if mask is not None:
                attn = attn[:, :, :-1, :-1]
            return x, attn_nosf
        else:
            return x    #原始注意力


# 用模板的q来计算
class MixedCrossAttention(Attention):

    def forward(self, x, len_t):
        # print(x.shape,'x.shape')#torch.Size([8, 320, 768]) x.shape

        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(
                self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        q_s = q[:, :, len_t:]

        #  if len_t ==0 :
        #   q_s=self.flatnn(q_s)

        # 9次template                             6次search                             3次融合 len_t-64
        # print(q.shape,'q,shape') #        torch.Size([8, 12, 64, 64]) q,shape   torch.Size([8, 12, 256, 64]) q,shape   torch.Size([8, 12, 320, 64]) q,shape
        # print(q_s.shape,'qs.shape')#      torch.Size([8, 12, 64, 64]) qs.shape  torch.Size([8, 12, 256, 64]) qs.shape  torch.Size([8, 12, 256, 64]) qs.shape
        attn = (q_s @ k.transpose(-2, -1)) * self.scale  # 这里是64是因为拼接后选取serach的q作为q
        # print(attn.shape,'atnn.shape')  #  torch.Size([8, 12, 64, 64]) atnn.shape  torch.Size([8, 12, 256, 256]) atnn.shape   torch.Size([8, 12, 256, 320]) atnn.shape
        attn = self.attn_drop(attn)

        attn_s2ts = attn  # 9次template                             6次search                             3次融合 len_t-64
        attn_s2ts = attn_s2ts.softmax(dim=-1)
        # print(x.shape,'x.shape')#        torch.Size([8, 64, 768]) x.shape   torch.Size([8, 256, 768]) x.shape    torch.Size([8, 320, 768]) x.shape
        x_t = x[:, :len_t]
        # print(x_t.shape, 'x_t.shape')   #torch.Size([8, 0, 768]) x_s.shape  torch.Size([8, 0, 768]) x_s.shape   torch.Size([8, 64, 768]) x_s.shape
        x_s = (attn_s2ts @ v).transpose(1, 2).reshape(B, N - len_t, C)
        # print(x_s.shape,'x_s.shape')    # torch.Size([8, 64, 768]) x_s.shape  torch.Size([8, 256, 768]) x_s.shape     torch.Size([8, 256, 768]) x_s.shape
        x = torch.cat((x_t, x_s), dim=1)
        #  print(x.shape,'x.shape')        #torch.Size([8, 64, 768]) x.shape      torch.Size([8, 256, 768]) x.shape       torch.Size([8, 320, 768]) x.shape
        x = self.proj(x)
        x = self.proj_drop(x)

        return x



class RASA(nn.Module):
    # print('rasa1')
    def __init__(
            self,
            dim, num_heads=8, qkv_bias=False,
            qk_scale=None, attn_drop=0.,
            proj_drop=0.,
            rasa_cfg=None, sr_ratio=1,
            linear=False,
    ):
        super().__init__()
        assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."

#调用参数，改递归次数
        rasa_cfg = dict(
            atrous_rates=[1, 3, 5],  # None, [1,3,5]
            act_layer='nn.SiLU(True)',
            init='kaiming',
            r_num=2,         #递归次数
        )
#

        self.dim = dim
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.q = nn.Linear(dim, dim, bias=qkv_bias)
        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        self.linear = linear
        self.rasa_cfg = rasa_cfg
        # print(self.rasa_cfg)
        self.use_rasa = rasa_cfg is not None
        self.sr_ratio = sr_ratio

        if not linear:
            if sr_ratio > 1:
                self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)
                self.norm = nn.LayerNorm(dim)
        else:
            self.pool = nn.AdaptiveAvgPool2d(7)
            self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1)
            self.norm = nn.LayerNorm(dim)
            self.act = nn.GELU()

        if self.use_rasa:
            if self.rasa_cfg['atrous_rates'] is not None:
                self.ds = ds_conv2d(
                    dim, dim, kernel_size=3, stride=1,
                    dilation=self.rasa_cfg['atrous_rates'], groups=dim, bias=qkv_bias,
                    act_layer=self.rasa_cfg['act_layer'], init=self.rasa_cfg['init'],
                )
            if self.rasa_cfg['r_num'] > 1:
                self.silu = nn.SiLU(True)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def _inner_attention(self, x):
        # print('rasa2')

        # N, B, C = x.shape  ##
        # x = x.permute(1, 0, 2)  ##N B C  ---> B N C
        # H = W = int(math.sqrt(N))  ##
        # x = x.reshape(B, H, W, C)  ##

        # N, B1, C = x.shape    ##
        # # print(x.shape)   #torch.Size([1452, 4, 512])
        # x=x.permute(1,0,2)   ##N B C  ---> B N C
        # # print(x.shape)   #torch.Size([4, 1452, 512])
        # n=int(N//int(N//(22*22)))
        # b= B1*int(N// (22 * 22))
        # H=W=int(n**0.5)
        # x = x.reshape(b, H, W, C)

        B, N, C = x.shape
        H = W = int(math.sqrt(N))  ##DIM   CHANNEL
        x = x.reshape(B, H, W, C)  ##

        #----------------------
        B, H, W, C = x.shape
        q = self.q(x).reshape(B, H * W, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)

        if self.use_rasa:
            if self.rasa_cfg['atrous_rates'] is not None:
                q = q.permute(0, 1, 3, 2).reshape(B, self.dim, H, W).contiguous()
                q = self.ds(q)
                q = q.reshape(B, self.num_heads, self.dim // self.num_heads, H * W).permute(0, 1, 3, 2).contiguous()

        if not self.linear:
            if self.sr_ratio > 1:
                x_ = x.permute(0, 3, 1, 2)
                x_ = self.sr(x_).permute(0, 2, 3, 1)
                x_ = self.norm(x_)
                kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            else:
                kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        else:
            raise NotImplementedError

        k, v = kv[0], kv[1]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, H, W, C)
        x = self.proj(x)
        x = self.proj_drop(x)


        #---------------------
        # x = x.reshape(B, N, C)  ##
        # x = x.permute(1, 0, 2)  ##B N C ---> N B C

        # x = x.reshape(B1, N, C)  ##
        # x = x.permute(1, 0, 2)  ##B N C ---> N B C
        x = x.reshape(B, N, C)

        return x

    def forward(self, x):
        # print('rasa4')
        if self.use_rasa:
            # print('rasa2')
            x_in = x
            x = self._inner_attention(x)
            if self.rasa_cfg['r_num'] > 1:
                # print('rasa1')
                x = self.silu(x)
            for _ in range(self.rasa_cfg['r_num']  - 1):
                # print('rasa3')
                x = x + x_in
                x_in = x
                x = self._inner_attention(x)
                x = self.silu(x)                      #silu实现自校准机制，通过激活强度来确定每个刻度的重量
        else:
            x = self._inner_attention(x)

        return x


class ds_conv2d(nn.Module):
    # print('rasa4')
    def __init__(self, in_planes, out_planes, kernel_size, stride=1,
                 dilation=[1, 3, 5], groups=1, bias=True,
                 act_layer='nn.SiLU(True)', init='kaiming'):
        super().__init__()
        assert in_planes % groups == 0
        assert kernel_size == 3, 'support kernel size 3 now'
        self.in_planes = in_planes
        self.out_planes = out_planes
        self.kernel_size = kernel_size
        self.stride = stride
        self.dilation = dilation
        self.groups = groups
        self.with_bias = bias

        self.weight = nn.Parameter(torch.randn(out_planes, in_planes // groups, kernel_size, kernel_size),
                                   requires_grad=True)
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_planes))
        else:
            self.bias = None
        self.act = eval(act_layer)
        self.init = init
        self._initialize_weights()

    def _initialize_weights(self):
        if self.init == 'dirac':
            nn.init.dirac_(self.weight, self.groups)
        elif self.init == 'kaiming':
            nn.init.kaiming_uniform_(self.weight)
        else:
            raise NotImplementedError
        if self.with_bias:
            if self.init == 'dirac':
                nn.init.constant_(self.bias, 0.)
            elif self.init == 'kaiming':
                bound = self.groups / (self.kernel_size ** 2 * self.in_planes)
                bound = math.sqrt(bound)
                nn.init.uniform_(self.bias, -bound, bound)
            else:
                raise NotImplementedError

    def forward(self, x):
        output = 0
        for dil in self.dilation:
            output += self.act(
                F.conv2d(
                    x, weight=self.weight, bias=self.bias, stride=self.stride, padding=dil,
                    dilation=dil, groups=self.groups,
                )
            )
        return output







class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., seperate_self_attn=False,
                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, window_size=None,
                 init_values=0,
                 add_cls_token=False):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.add_cls_token = add_cls_token
        self.drop_path = DropPath(
            drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=act_layer, drop=drop)
        self.attn = MixedCrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
                                        attn_drop=attn_drop, proj_drop=drop, window_size=window_size)


        self.attn2 = RASA(dim)      #改成新的注意力  1111


        if init_values > 0:
            self.gamma_1 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True)
            self.gamma_2 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True)
        else:
            self.gamma_1, self.gamma_2 = None, None



    def forward(self, x, len_t):

        # Here we keep the template part unchanged
        x_t = x[:, :len_t]
        x_s = x[:, len_t:]
        x = torch.cat((x_t, x_s), dim=1)
        if len_t == 0:
            y = self.attn2(self.norm1(x))
        else:
            y = self.attn(self.norm1(x), len_t)

        y_s = y[:, len_t:]

        if self.gamma_1 is None:
            x = x + self.drop_path(y)
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        else:
            x_s = x_s + self.drop_path(self.gamma_1 * y_s)
            x_s = x_s + self.drop_path(self.gamma_2 *
                                       self.mlp(self.norm2(x_s)))
            x = torch.cat((x_t, x_s), dim=1)
        return x


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """

    def __init__(self, img_size=[224, 224], patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.num_patches_w = img_size[0] // patch_size
        self.num_patches_h = img_size[1] // patch_size

        num_patches = self.num_patches_w * self.num_patches_h
        self.patch_shape = (img_size[0] // patch_size,
                            img_size[1] // patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        # print("##############patch here!!!")
        self.proj = nn.Conv2d(in_chans, embed_dim,
                              kernel_size=patch_size, stride=patch_size)
        self.flatten = True

    def forward(self, x, mask=None):
        # B, C, H, W = x.shape
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
        # x = self.norm(x)
        return x


class VIT_Backbone(nn.Module):
    def __init__(self,
                 img_size=[224, 224],
                 patch_size=16,
                 embed_dim=768,
                 in_chans=3,
                 frozen_stages=-1,
                 #  out_indices=[3, 5, 7, 11],
                 out_with_norm=False,
                 use_checkpoint=False,
                 num_async_interaction_stage=3,
                 add_cls_token=False,
                 add_target_token=True,
                 num_classes=0,
                 depth=12,
                 num_heads=12,
                 mlp_ratio=4.,
                 qkv_bias=False,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.,
                 norm_layer=partial(nn.LayerNorm, eps=1e-6),
                 return_all_tokens=False,
                 init_values=0,
                 use_sincos_pos_emb=False,
                 use_abs_pos_emb=False,
                 use_rel_pos_bias=False,
                 seperate_loc=None,
                 act_layer=None,
                 ce_loc=None,
                 ce_keep_ratio=None,
                 use_mean_pooling=False,
                 masked_im_modeling=False,
                 ):

        super().__init__()

        self.num_features = self.embed_dim = embed_dim
        self.return_all_tokens = return_all_tokens
        self.use_abs_pos_emb = use_abs_pos_emb
        self.use_sincos_pos_emb = use_sincos_pos_emb
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        if use_abs_pos_emb:
            self.pos_embed = nn.Parameter(
                torch.zeros(1, num_patches + 1, embed_dim))
        else:
            self.pos_embed = None

        self.pos_drop = nn.Dropout(p=drop_rate)
        act_layer = act_layer or nn.GELU
        self.use_rel_pos_bias = use_rel_pos_bias

        if self.use_rel_pos_bias:
            print("=================use RelativePositionBias===================")
            window_size = self.patch_embed.patch_shape
            self.window_size = window_size
            self.num_relative_distance = (
                2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3
            self.relative_position_bias_table = nn.Parameter(
                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH
            # cls to token & token 2 cls & cls to cls

        else:
            self.window_size = None
            self.relative_position_bias_table = None
            self.relative_position_index = None

        # stochastic depth decay rule
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]

        blocks = []

        for i in range(depth):
            blocks.append(Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[
                    i], norm_layer=norm_layer,
                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None))
        self.blocks = nn.ModuleList(blocks)

        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)
        if use_abs_pos_emb:
            trunc_normal_(self.pos_embed, std=.02)
        trunc_normal_(self.cls_token, std=.02)
        self.apply(self._init_weights)

        # masked image modeling
        self.masked_im_modeling = masked_im_modeling
        if masked_im_modeling:
            self.masked_embed = nn.Parameter(torch.zeros(1, embed_dim))

        # support non-square image as input
        if len(img_size) == 1:
            img_size = img_size * 2
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches
        if self.use_abs_pos_emb:
            self.pos_embed = nn.Parameter(
                torch.zeros(1, num_patches + 1, embed_dim))
        elif self.use_sincos_pos_emb:
            self.pos_embed = self.build_2d_sincos_position_embedding(embed_dim)
        else:
            self.pos_embed = None

        self.add_target_token = add_target_token
        self.patch_size = patch_size
        self.frozen_stages = frozen_stages
        self.use_checkpoint = use_checkpoint
        self.num_FE_layers = depth - num_async_interaction_stage
        self.depth = depth
        # self.num_async_interaction_stage = num_async_interaction_stage

        self.pos_embed_z = None
        self.pos_embed_x = None

        if not out_with_norm:
            self.norm = nn.Identity()

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def finetune_track(self, cfg, patch_start_index=1):
        # patch_start_index: For skipping the cls embedding
        search_size = to_2tuple(cfg.DATA.SEARCH.SIZE)
        template_size = to_2tuple(cfg.DATA.TEMPLATE.SIZE)
        new_patch_size = cfg.MODEL.BACKBONE.STRIDE

        # resize patch embedding
        if new_patch_size != self.patch_size:
            print(
                'Inconsistent Patch Size With The Pretrained Weights, Interpolate The Weight!')
            old_patch_embed = {}
            for name, param in self.patch_embed.named_parameters():
                if 'weight' in name:
                    param = nn.functional.interpolate(param, size=(new_patch_size, new_patch_size),
                                                      mode='bicubic', align_corners=False)
                    param = nn.Parameter(param)
                old_patch_embed[name] = param
            # self.patch_embed = PatchEmbed(img_size=self.img_size, patch_size=new_patch_size, in_chans=3,
            #                               embed_dim=self.embed_dim)
            self.patch_embed.proj.bias = old_patch_embed['proj.bias']
            self.patch_embed.proj.weight = old_patch_embed['proj.weight']

        # Pos embedding
        patch_pos_embed = self.pos_embed[:, patch_start_index:, :]
        patch_pos_embed = patch_pos_embed.transpose(1, 2)  # ?
        B, E, Q = patch_pos_embed.shape
        P_H, P_W = self.patch_embed.patch_shape[0], self.patch_embed.patch_shape[1]
        patch_pos_embed = patch_pos_embed.view(B, E, P_H, P_W)

        # Pos embedding for search region
        H, W = search_size
        new_P_H_S, new_P_W_S = H // new_patch_size, W // new_patch_size
        search_patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H_S, new_P_W_S), mode='bicubic',
                                                           align_corners=False)
        search_patch_pos_embed = search_patch_pos_embed.flatten(
            2).transpose(1, 2)

        # Pos embedding for template region
        H, W = template_size
        new_P_H_T, new_P_W_T = H // new_patch_size, W // new_patch_size
        template_patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H_T, new_P_W_T),
                                                             mode='bicubic',
                                                             align_corners=False)
        template_patch_pos_embed = template_patch_pos_embed.flatten(
            2).transpose(1, 2)

        self.pos_embed_z = nn.Parameter(template_patch_pos_embed)
        self.pos_embed_x = nn.Parameter(search_patch_pos_embed)

    def train(self, mode=True):
        """Convert the model into training mode while keep layers freezed."""
        super(VIT_Backbone, self).train(mode)
        self._freeze_stages()

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for param in self.patch_embed.parameters():
                param.requires_grad = False
            self.cls_token.requires_grad = False
            if self.pos_embed is not None and self.use_sincos_pos_emb == True:
                self.pos_embed.requires_grad = False
            self.pos_drop.eval()

        for i in range(1, self.frozen_stages + 1):

            if i == len(self.blocks):
                norm_layer = getattr(self, 'norm')  # f'norm{i-1}')
                norm_layer.eval()
                for param in norm_layer.parameters():
                    param.requires_grad = False

            m = self.blocks[i - 1]
            m.eval()
            for param in m.parameters():
                param.requires_grad = False


    def forward_zx(self, z, x, target_token=None):
        x = self.patch_embed(x)
        x += self.pos_embed_x
        x = self.pos_drop(x)

        len_z = self.pos_embed_z.shape[1]

        if self.add_target_token and target_token is not None:
            z = torch.cat([target_token, z], dim=1)
        # print(self.num_FE_layers)
        for i in range(self.num_FE_layers):  # 6
            # for i in range(3):  # 6
            # print('2') #6次serach
            x = self.blocks[i](x, 0)

        if self.add_target_token and target_token is not None:
            z = z[:, 1:]

        x = torch.cat((z, x), dim=1)
        # print(self.depth)
        for i in range(self.num_FE_layers, self.depth):  # 6-9
            # print('1') #3次融合
            x = self.blocks[i](x, len_z)

        x = self.norm(x)  # torch.Size([8, 320, 768]) x

        return x

    def forward_z(self,
                  z,
                  target_token=None,
                  ):

        z = self.patch_embed(z)
        z += self.pos_embed_z
        z = self.pos_drop(z)

        # if self.add_target_token and target_token is not None:
        #     z = torch.cat([target_token, z], dim=1)

        for i, blk in enumerate(self.blocks):
            #  print('')#9次template
            z = blk(z, 0)

        # if self.add_target_token and target_token is not None:
        #     z = z[:, 1:]  # torch.Size([8, 64, 768]) z

        return z

    def forward(self, z, x=None,
                target_token=None,
                mode=None
                ):
        if mode is None:
            # print('xz')
            # z=tempalte  x=serach

            z_feat = self.forward_z(z)
            x_feat = self.forward_zx(z_feat, x, target_token=target_token)
        elif mode == 'x':

            x_feat = self.forward_zx(z, x)
        elif mode == 'z':

            x_feat = self.forward_z(z, target_token=target_token)

        return x_feat


def _create_ViT_CAE(pretrained=False, **kwargs) -> VIT_Backbone:
    model = VIT_Backbone(**kwargs)

    if pretrained:
        if 'npz' in pretrained:
            model.load_pretrained(pretrained, prefix='')
        else:
            state_dict = torch.load(
                '/home/wangjun/code/LiteTrack-main/pretrained_models/cae_base.pth', map_location="cpu")

            # new_ckpt = OrderedDict()
            state_dict = state_dict['model']
            if sorted(list(state_dict.keys()))[0].startswith('encoder'):
                state_dict = {k.replace(
                    'encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}
            missing_keys, unexpected_keys = model.load_state_dict(
                state_dict, strict=False)
            print("Unexpected keys:")
            print(unexpected_keys)
            print("Missing keys:")
            print(missing_keys)
    return model


def CAE_Base_patch16_224_Async(pretrained=False, **kwargs) -> VIT_Backbone:
    model_kwargs = dict(mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),
                        patch_size=16, embed_dim=768, num_heads=12,
                        drop_rate=0., attn_drop_rate=0., init_values=0.1,
                        use_abs_pos_emb=True, **kwargs)
    model = _create_ViT_CAE(pretrained=pretrained, **model_kwargs)
    # model.adpat4track()
    return model
